apiVersion: ray.io/v1
kind: RayJob
metadata:
  name: distributed-training
  namespace: ray
spec:
  entrypoint: python /home/ray/scripts/distributed_training.py
  runtimeEnvYAML: |
    pip:
      - ipywidgets==8.1.3
      - matplotlib==3.10.0
      - mlflow==2.19.0
      - torch==2.7.1
      - transformers==4.52.3
      - scikit-learn==1.6.0
      - "doggos @ git+https://github.com/anyscale/multimodal-ai.git#subdirectory=doggos"
    env_vars:
      RAY_TRAIN_V2_ENABLED: "1"

  shutdownAfterJobFinishes: false   # keep cluster alive to inspect results
  # ttlSecondsAfterFinished: 3600   # clean up 1 h after completion

  rayClusterSpec:
    rayVersion: "2.48.0"
    enableInTreeAutoscaling: false

    headGroupSpec:
      rayStartParams:
        num-cpus: "0"
        dashboard-host: "0.0.0.0"
        disable-usage-stats: "true"
      template:
        spec:
          nodeSelector:
            agentpool: system
          containers:
            - name: ray-head
              image: rayproject/ray:2.48.0-py312
              env:
                - name: RAY_DISABLE_DOCKER_CPU_WARNING
                  value: "1"
                - name: RAY_SCHEDULER_EVENTS
                  value: "0"
                - name: RAY_TRAIN_V2_ENABLED
                  value: "1"
              ports:
                - containerPort: 6379
                  name: gcs-server
                - containerPort: 8265
                  name: dashboard
                - containerPort: 10001
                  name: client
              resources:
                limits:
                  cpu: "8"
                  memory: "32Gi"
                requests:
                  cpu: "8"
                  memory: "32Gi"
              volumeMounts:
                - name: scripts
                  mountPath: /home/ray/scripts
                - name: cluster-storage
                  mountPath: /mnt/cluster_storage
          volumes:
            - name: scripts
              configMap:
                name: distributed-training-scripts
                items:
                  - key: distributed_training.py
                    path: distributed_training.py
            - name: cluster-storage
              persistentVolumeClaim:
                claimName: cluster-storage

    workerGroupSpecs:
      - groupName: gpu-workers
        replicas: 2
        minReplicas: 2
        maxReplicas: 2
        rayStartParams:
          num-gpus: "4"
          num-cpus: "64"
        template:
          spec:
            containers:
              - name: ray-worker
                image: rayproject/ray:2.48.0-py312-gpu
                env:
                  - name: RAY_TRAIN_V2_ENABLED
                    value: "1"
                resources:
                  limits:
                    cpu: "64"
                    memory: "64Gi"
                    nvidia.com/gpu: "4"
                  requests:
                    cpu: "64"
                    memory: "64Gi"
                    nvidia.com/gpu: "4"
                volumeMounts:
                  - name: scripts
                    mountPath: /home/ray/scripts
                  - name: cluster-storage
                    mountPath: /mnt/cluster_storage
            volumes:
              - name: scripts
                configMap:
                  name: distributed-training-scripts
                  items:
                    - key: distributed_training.py
                      path: distributed_training.py
              - name: cluster-storage
                persistentVolumeClaim:
                  claimName: cluster-storage
